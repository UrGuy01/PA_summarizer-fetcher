{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae222ed8",
   "metadata": {},
   "source": [
    "## Document Loaders In LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dada0",
   "metadata": {},
   "source": [
    "#### TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470ed0a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'knowledge_base.txt'}, page_content='Getting Started with Large Language Models (LLMs): A Comprehensive Roadmap\\n\\nLarge Language Models (LLMs) such as GPT-4, Claude, LLaMA, and Mistral are at the forefront of artificial intelligence, capable of understanding, generating, and reasoning with human language. They are transforming industries from customer service to scientific research. However, the breadth and depth of this field can feel overwhelming to a beginner. The key to starting your journey in LLMs lies in approaching the subject systematically, balancing theoretical understanding with hands-on practice, and staying up to date with the rapid pace of development. This guide outlines exactly how to do that.\\n\\n1. Build a Strong Foundation in Prerequisites\\n\\nBefore you dive into LLMs, ensure you have a solid grounding in three core areas:\\n\\nPython Programming\\nPython is the de facto language for AI development due to its simplicity and the vast ecosystem of machine learning libraries. You should be comfortable with:\\n\\nWriting and debugging scripts\\n\\nWorking with libraries like numpy, pandas, and matplotlib\\n\\nReading documentation and adapting sample code\\nRecommended resources: Automate the Boring Stuff with Python, Python Crash Course, and practice on platforms like LeetCode or HackerRank.\\n\\nMathematics for Machine Learning\\nWhile you don’t need a PhD in math, understanding the fundamentals will make you far more effective:\\n\\nLinear Algebra: vectors, matrices, dot products, matrix multiplication\\n\\nProbability & Statistics: distributions, expectations, Bayes’ theorem\\n\\nCalculus: derivatives, gradients (especially for optimization in neural networks)\\n\\nOptimization: gradient descent and its variants\\nRecommended resource: Mathematics for Machine Learning by Deisenroth, Faisal, and Ong.\\n\\nMachine Learning Fundamentals\\nUnderstand the difference between supervised, unsupervised, and reinforcement learning. Study:\\n\\nBasic algorithms (linear regression, logistic regression, decision trees, k-means)\\n\\nOverfitting, underfitting, regularization\\n\\nEvaluation metrics (accuracy, precision, recall, F1-score)\\nRecommended course: Andrew Ng’s Machine Learning Specialization (Coursera).\\n\\n2. Learn the Deep Learning Foundations\\n\\nLLMs are deep learning models, so you need to be fluent in the concepts behind neural networks:\\n\\nArtificial Neural Networks (ANNs)\\nLearn how neurons, layers, and activation functions work. Understand forward and backward propagation.\\n\\nConvolutional and Recurrent Networks\\nEven though transformers have largely replaced these in NLP, understanding CNNs and RNNs will give you historical and conceptual grounding.\\n\\nTransformers\\nTransformers are the architecture behind all modern LLMs. Study:\\n\\nThe Attention Mechanism: how models focus on relevant parts of the input\\n\\nPositional encoding\\n\\nEncoder-decoder vs. decoder-only architectures\\nRecommended reading: Attention is All You Need (Vaswani et al., 2017) and illustrated explanations like The Illustrated Transformer by Jay Alammar.\\n\\nFrameworks\\nGet comfortable with PyTorch or TensorFlow, as they are the backbone of LLM experimentation. PyTorch is often preferred for research and rapid prototyping.\\n\\n3. Dive into Natural Language Processing (NLP)\\n\\nNLP knowledge bridges the gap between generic deep learning and LLM applications:\\n\\nText Preprocessing\\nTokenization, stemming, lemmatization, stopword removal.\\n\\nWord Representations\\nStudy the evolution from:\\n\\nBag-of-Words (BoW)\\n\\nTF-IDF\\n\\nWord embeddings like Word2Vec, GloVe, and FastText\\n\\nContextual embeddings from models like ELMo, BERT\\n\\nCore NLP Tasks\\nSentiment analysis, named entity recognition (NER), text classification, summarization, question answering.\\n\\nEvaluation in NLP\\nLearn about BLEU, ROUGE, and perplexity metrics.\\n\\n4. Understand LLMs and Their Ecosystem\\n\\nOnce the foundations are in place, move into LLM-specific knowledge:\\n\\nPretraining vs. Fine-tuning\\nUnderstand how LLMs are trained on vast corpora (pretraining) and then adapted for specific tasks (fine-tuning or instruction-tuning).\\n\\nPrompt Engineering\\nLearn how to write effective prompts, chain prompts, and use few-shot examples to steer the model.\\n\\nRetrieval-Augmented Generation (RAG)\\nCombine LLMs with external knowledge bases to answer questions using up-to-date or domain-specific data.\\n\\nPopular LLM Frameworks\\n\\nLangChain: for building LLM applications with tools, memory, and chaining\\n\\nLlamaIndex: for connecting LLMs to structured and unstructured data sources\\n\\nTransformers Library (Hugging Face): for loading, fine-tuning, and deploying LLMs\\n\\nDeployment & APIs\\nLearn how to:\\n\\nUse hosted APIs (OpenAI, Anthropic, Cohere)\\n\\nDeploy local models via Hugging Face or Ollama\\n\\nIntegrate into web apps (Flask, FastAPI, Streamlit)\\n\\n5. Develop Practical Projects\\n\\nTheory without practice leads to shallow understanding. Apply your knowledge through projects:\\n\\nBeginner:\\n\\nChatbot using OpenAI’s API\\n\\nText summarizer with Hugging Face models\\n\\nSentiment analysis pipeline\\n\\nIntermediate:\\n\\nRAG chatbot connected to a PDF knowledge base\\n\\nSemantic search engine\\n\\nEmail assistant that drafts replies\\n\\nAdvanced:\\n\\nFine-tune a LLaMA or Mistral model for domain-specific Q&A\\n\\nMulti-agent systems for task automation\\n\\nDeploy LLM in a scalable API service\\n\\n6. Stay Updated with the Fast-Moving Field\\n\\nLLMs evolve at lightning speed. Keep learning by:\\n\\nReading research papers on arXiv (look for “transformer”, “LLM”, “instruction tuning”).\\n\\nFollowing AI newsletters (The Batch by deeplearning.ai, Hugging Face Weekly).\\n\\nWatching conference talks (NeurIPS, ACL, ICML).\\n\\nExperimenting with new models on Hugging Face Spaces.\\n\\n7. Suggested Learning Path Timeline\\n\\nHere’s a realistic progression for 6–9 months of part-time study:\\n\\nMonth 1–2: Python + ML math + ML basics\\n\\nMonth 3: Deep learning + PyTorch\\n\\nMonth 4: NLP fundamentals\\n\\nMonth 5: Transformers and LLM architectures\\n\\nMonth 6: Prompt engineering + RAG\\n\\nMonth 7–9: Advanced projects + fine-tuning\\n\\n8. Mindset and Strategy for Mastery\\n\\nFinally, remember:\\n\\nIterative learning beats perfection — build small things quickly, then refine.\\n\\nDocumentation is your friend — Hugging Face, PyTorch, LangChain docs are gold mines.\\n\\nCommunity engagement accelerates growth — join Discord servers, GitHub discussions, or Kaggle competitions.\\n\\nBalance breadth with depth — explore widely, then specialize in an area (e.g., fine-tuning, deployment, multimodal LLMs).\\n\\nIf you commit to this roadmap, you’ll go from beginner to building your own LLM-powered applications with confidence and a deep understanding of how they work under the hood.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"knowledge_base.txt\",encoding ='UTF-8')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5844b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.document_loaders.text.TextLoader"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb95e130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knowledge_base.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da25ff7",
   "metadata": {},
   "source": [
    "#### CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad67ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220b3fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'models.csv', 'row': 0}, page_content='Model Name: GPT-4\\nProvider: OpenAI\\nRelease Date: 2023-03\\nParameters: >1T\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multimodal (text + image)\\nNone: better reasoning,long context'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 1}, page_content='Model Name: GPT-3.5\\nProvider: OpenAI\\nRelease Date: 2022-11\\nParameters: 175B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2021-09\\nKey Features: Faster and cheaper than GPT-3\\nNone: improved chat'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 2}, page_content='Model Name: Claude 2\\nProvider: Anthropic\\nRelease Date: 2023-07\\nParameters: Unknown\\nArchitecture: Constitutional AI\\nTraining Data Cutoff: 2023-01\\nKey Features: Long context window (100k tokens)\\nNone: safety-focused'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 3}, page_content='Model Name: Claude Instant\\nProvider: Anthropic\\nRelease Date: 2023-03\\nParameters: Unknown\\nArchitecture: Constitutional AI\\nTraining Data Cutoff: 2023-01\\nKey Features: Faster\\nNone: cheaper Claude variant'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 4}, page_content='Model Name: LLaMA 2\\nProvider: Meta\\nRelease Date: 2023-07\\nParameters: 7B / 13B / 70B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-06\\nKey Features: Open-source\\nNone: strong fine-tuning ability'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 5}, page_content='Model Name: PaLM 2\\nProvider: Google\\nRelease Date: 2023-05\\nParameters: Unknown\\nArchitecture: Pathways Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multilingual\\nNone: strong reasoning in math/code'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 6}, page_content='Model Name: Gemini 1.5\\nProvider: Google DeepMind\\nRelease Date: 2024-02\\nParameters: Unknown\\nArchitecture: Multimodal Transformer\\nTraining Data Cutoff: 2024-01\\nKey Features: Very long context window (1M tokens)'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 7}, page_content='Model Name: Mistral 7B\\nProvider: Mistral AI\\nRelease Date: 2023-09\\nParameters: 7B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-08\\nKey Features: Efficient\\nNone: open-weight,competitive performance'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 8}, page_content='Model Name: Mixtral\\nProvider: Mistral AI\\nRelease Date: 2023-12\\nParameters: 46.7B (8x7B Mixture-of-Experts)\\nArchitecture: MoE Transformer\\nTraining Data Cutoff: 2023-11\\nKey Features: Open-weight\\nNone: better efficiency'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 9}, page_content='Model Name: Command R+\\nProvider: Cohere\\nRelease Date: 2024-04\\nParameters: Unknown\\nArchitecture: Transformer\\nTraining Data Cutoff: 2024-02\\nKey Features: RAG-optimized\\nNone: strong retrieval abilities'),\n",
       " Document(metadata={'source': 'models.csv', 'row': 10}, page_content='Model Name: Falcon 180B\\nProvider: Technology Innovation Institute\\nRelease Date: 2023-09\\nParameters: 180B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-07\\nKey Features: Open-weight\\nNone: high accuracy')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"models.csv\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1b3b9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'models.csv', 'row': 0}, page_content='Model Name: GPT-4\\nProvider: OpenAI\\nRelease Date: 2023-03\\nParameters: >1T\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multimodal (text + image)\\nNone: better reasoning,long context')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e356cd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'GPT-4', 'row': 0}, page_content='Model Name: GPT-4\\nProvider: OpenAI\\nRelease Date: 2023-03\\nParameters: >1T\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multimodal (text + image)\\nNone: better reasoning,long context'),\n",
       " Document(metadata={'source': 'GPT-3.5', 'row': 1}, page_content='Model Name: GPT-3.5\\nProvider: OpenAI\\nRelease Date: 2022-11\\nParameters: 175B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2021-09\\nKey Features: Faster and cheaper than GPT-3\\nNone: improved chat'),\n",
       " Document(metadata={'source': 'Claude 2', 'row': 2}, page_content='Model Name: Claude 2\\nProvider: Anthropic\\nRelease Date: 2023-07\\nParameters: Unknown\\nArchitecture: Constitutional AI\\nTraining Data Cutoff: 2023-01\\nKey Features: Long context window (100k tokens)\\nNone: safety-focused'),\n",
       " Document(metadata={'source': 'Claude Instant', 'row': 3}, page_content='Model Name: Claude Instant\\nProvider: Anthropic\\nRelease Date: 2023-03\\nParameters: Unknown\\nArchitecture: Constitutional AI\\nTraining Data Cutoff: 2023-01\\nKey Features: Faster\\nNone: cheaper Claude variant'),\n",
       " Document(metadata={'source': 'LLaMA 2', 'row': 4}, page_content='Model Name: LLaMA 2\\nProvider: Meta\\nRelease Date: 2023-07\\nParameters: 7B / 13B / 70B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-06\\nKey Features: Open-source\\nNone: strong fine-tuning ability'),\n",
       " Document(metadata={'source': 'PaLM 2', 'row': 5}, page_content='Model Name: PaLM 2\\nProvider: Google\\nRelease Date: 2023-05\\nParameters: Unknown\\nArchitecture: Pathways Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multilingual\\nNone: strong reasoning in math/code'),\n",
       " Document(metadata={'source': 'Gemini 1.5', 'row': 6}, page_content='Model Name: Gemini 1.5\\nProvider: Google DeepMind\\nRelease Date: 2024-02\\nParameters: Unknown\\nArchitecture: Multimodal Transformer\\nTraining Data Cutoff: 2024-01\\nKey Features: Very long context window (1M tokens)'),\n",
       " Document(metadata={'source': 'Mistral 7B', 'row': 7}, page_content='Model Name: Mistral 7B\\nProvider: Mistral AI\\nRelease Date: 2023-09\\nParameters: 7B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-08\\nKey Features: Efficient\\nNone: open-weight,competitive performance'),\n",
       " Document(metadata={'source': 'Mixtral', 'row': 8}, page_content='Model Name: Mixtral\\nProvider: Mistral AI\\nRelease Date: 2023-12\\nParameters: 46.7B (8x7B Mixture-of-Experts)\\nArchitecture: MoE Transformer\\nTraining Data Cutoff: 2023-11\\nKey Features: Open-weight\\nNone: better efficiency'),\n",
       " Document(metadata={'source': 'Command R+', 'row': 9}, page_content='Model Name: Command R+\\nProvider: Cohere\\nRelease Date: 2024-04\\nParameters: Unknown\\nArchitecture: Transformer\\nTraining Data Cutoff: 2024-02\\nKey Features: RAG-optimized\\nNone: strong retrieval abilities'),\n",
       " Document(metadata={'source': 'Falcon 180B', 'row': 10}, page_content='Model Name: Falcon 180B\\nProvider: Technology Innovation Institute\\nRelease Date: 2023-09\\nParameters: 180B\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-07\\nKey Features: Open-weight\\nNone: high accuracy')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"models.csv\", source_column=\"Model Name\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d13fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model Name: GPT-4\\nProvider: OpenAI\\nRelease Date: 2023-03\\nParameters: >1T\\nArchitecture: Transformer\\nTraining Data Cutoff: 2023-01\\nKey Features: Multimodal (text + image)\\nNone: better reasoning,long context'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8913df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'GPT-4', 'row': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0163924",
   "metadata": {},
   "source": [
    "#### UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48644959",
   "metadata": {},
   "source": [
    "UnstructuredURLLoader of Langchain internally uses unstructured python library to load the content from url's\n",
    "\n",
    "https://unstructured-io.github.io/unstructured/introduction.html\n",
    "\n",
    "https://pypi.org/project/unstructured/#description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e1fbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing necessary libraries, libmagic is used for file type detection\n",
    "!pip3 install unstructured libmagic python-magic python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ce75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://aeon.co/essays/philosophy-in-prison-is-a-rowdy-honest-and-hopeful-provocation\",\n",
    "        \"https://aeon.co/essays/the-great-myth-of-empire-collapse\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be914bfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52838711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PhilosophySciencePsychologySocietyCulture\\n\\nEssaysVideos Popular About\\n\\nDONATENEWSLETTER\\n\\nMenu\\n\\nDonat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b587350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://aeon.co/essays/philosophy-in-prison-is-a-rowdy-honest-and-hopeful-provocation'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45adf4",
   "metadata": {},
   "source": [
    "## Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2e736",
   "metadata": {},
   "source": [
    "Why do we need text splitters in first place?\n",
    "\n",
    "LLM's have token limits. Hence we need to split the text which can be large into small chunks so that each chunk size is under the token limit. There are various text splitter classes in langchain that allows us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90431c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking some random text from wikipedia\n",
    "\n",
    "text = \"\"\"Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \n",
    "Kip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \n",
    "It has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9a95b",
   "metadata": {},
   "source": [
    "#### Manual approach of splitting the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a51fc70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher N'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say LLM token limit is 100, in that case we can do simple thing such as this\n",
    "\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2adae99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well but we want complete words and want to do this for entire text, may be we can use Python's split funciton\n",
    "\n",
    "words = text.split(\" \")\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ec5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "s = \"\"\n",
    "for word in words:\n",
    "    s += word + \" \"\n",
    "    if len(s)>200:\n",
    "        chunks.append(s)\n",
    "        s = \"\"\n",
    "        \n",
    "chunks.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d902bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt ',\n",
       " 'Damon, and Michael Caine. \\nSet in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06ebc4",
   "metadata": {},
   "source": [
    "**Splitting data into chunks can be done in native python but it is a tidious process. Also if necessary, you may need to experiment with various delimiters in an iterative manner to ensure that each chunk does not exceed the token length limit of the respective LLM.**\n",
    "\n",
    "**Langchain provides a better way through text splitter classes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b2909",
   "metadata": {},
   "source": [
    "#### Using Text Splitter Classes from Langchain\n",
    "\n",
    "#### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9505bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d86bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 358, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e027b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "120\n",
      "210\n",
      "181\n",
      "197\n",
      "207\n",
      "128\n",
      "357\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc73da3",
   "metadata": {},
   "source": [
    "As you can see, all though we gave 200 as a chunk size since the split was based on \\n, it ended up creating chunks that are bigger than size 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f3a10",
   "metadata": {},
   "source": [
    "Another class from Langchain can be used to recursively split the text based on a list of separators. This class is RecursiveTextSplitter. Let's see how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a61cf1",
   "metadata": {},
   "source": [
    "#### RecursiveTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dacf5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \\nSet in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\\n\\nBrothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \\nKip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \\nCinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \\nInterstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\\n\\nInterstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \\nIt has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "848eae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 200,  # size of each chunk created\n",
    "    chunk_overlap  = 0,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1151c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "120\n",
      "199\n",
      "10\n",
      "181\n",
      "197\n",
      "198\n",
      "8\n",
      "128\n",
      "191\n",
      "165\n",
      "198\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "chunks = r_splitter.split_text(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32135f4d",
   "metadata": {},
   "source": [
    "**Let's understand how exactly it formed these chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57ef6974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \\nSet in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_split = text.split(\"\\n\\n\")[0]\n",
    "first_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bc7719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1018d24",
   "metadata": {},
   "source": [
    "Recursive text splitter uses a list of separators, i.e.  separators = [\"\\n\\n\", \"\\n\", \".\"]\n",
    "\n",
    "So now it will first split using \\n\\n and then if the resulting chunk size is greater than the chunk_size parameter which is 200\n",
    "in our case, then it will use the next separator which is \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "739cef71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. ',\n",
       " 'It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. ',\n",
       " 'Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_split = first_split.split(\"\\n\")\n",
    "second_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "903f5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "121\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "for split in second_split:\n",
    "    print(len(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e69a7",
   "metadata": {},
   "source": [
    "Third split exceeds chunk size 200. Now it will further try to split that using the third separator which is ' ' (space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f4da9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_split[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda659a",
   "metadata": {},
   "source": [
    "When you split this using space (i.e. second_split[2].split(\" \")), it will separate out each word and then it will merge those \n",
    "chunks such that their size is close to 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv.rockyfinance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
